{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA with Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "3QWy1fECXnT2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Question answering with TensorFlow\n",
        "\n",
        "질문-답변(QA) 시스템은 자연언어형태로 이뤄진 질문에 답할 수 있도록 설계된 시스템을 말합니다. 몇몇 QA 시스템은 특정 질문에 답변하기 위해서 문구나 이미지 같은 원천으로부터 정보를 가져옵니다. 이런 “원천기반의” 시스템은 크게 두 개의 세부 카테고리로 나눠볼 수 있습니다. 하나는 개방형으로, 질문의 형태가 어떤 것이든 상관없지만, 그렇다고 특정 주제에 초점을 맞추고 있지 않은 형태가 있고, 또 다른 형태는 폐쇄형으로, 질문 형태가 세세한 제한을 가지고 있는데 그 제한이 사실 이미 정의된 원천과 관련 있는 형태입니다. (예를 들어 의약과 같이 특정 분야나 사전에 제공된 문구 같이 말입니다.)\n",
        "\n",
        " \n",
        "이번 글에서는 TensorFlow를 활용해서 질문-답변 시스템을 만들고 코딩하는 과정을 여러분에게 소개할 것입니다. 우리는 신경망 기반의 폐쇄형 원천을 가진 QA 시스템을 만들 것입니다. 이 일을 하기 위해서는, Kumar가 그들의 논문인 “Ask Me Anything: Dynamic Memory Networks for Natural Language Processing.” 에서 소개한 Dynamic Memory Network(DMN)이라고 알려진 모델의 간소화된 형태를 사용할 것입니다.\n",
        "\n",
        " \n",
        "\n",
        "시작하기에 앞서\n",
        "\n",
        " \n",
        "\n",
        "Python 3기반의 TensorFlow 1.2버전을 설치하는 것에 덧붙여서 다음의 것들이 설치되었는지 확인하시기 바랍니다.\n",
        "\n",
        " \n",
        "\n",
        "* Jupyter\n",
        "* Numpy\n",
        "* Matplotlib\n",
        " \n",
        "\n",
        "부가적으로 학습과정을 보고 학습 속도 계수를 뽑기 위한 도구로 TQDM을 설치할 수 있겠지만 필수적이지는 않습니다. 이번 글을 위한 코드와 Jupyter Notebook은 GitHub에 올라가 있고, 한번 여러분들도 받아서 따라해 볼 것을 권합니다. 만약 TensorFlow를 처음 다뤄본다면, Aaron Schumacher가 TensorFlow가 무엇이고, 동작원리에 대해서 쓴 “Hello, TensorFlow”을 볼 것을 추천합니다. 만약 자연언어처리에 TensorFlow를 처음 써본다면, “Textual Entailment with TensorFlow”를 확인해볼 것을 권하는데, 이 글은 이 신경망을 만드는데 도움이 될 몇 가지 개념에 대해서 소개하고 있습니다.\n",
        "\n",
        " \n",
        "\n",
        "이제 이와 관련된 라이브러리를 추가하는 것부터 시작해봅시다.# Question answering with TensorFlow\n"
      ]
    },
    {
      "metadata": {
        "id": "e2sP1NA-cpS5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "75bfb656-0740-47a2-e7e3-8abdcd105267"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package libfuse2:amd64.\n",
            "(Reading database ... 26397 files and directories currently installed.)\n",
            "Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package fuse.\n",
            "Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking fuse (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu2~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu2~ubuntu18.04.1) ...\n",
            "Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up fuse (2.9.7-1ubuntu1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu2~ubuntu18.04.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZfW8Ic7zXnT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import urllib\n",
        "import sys\n",
        "import os\n",
        "import zipfile\n",
        "import tarfile\n",
        "import json \n",
        "import hashlib\n",
        "import re\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_xm02b-oXnT9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GloVe를 Parsing하고 미지의 토큰 다루기\n",
        " \n",
        "\n",
        "“TensorFlow를 활용한 텍스트 동조” 편에서, 저는 sentence2sequence라는 API를 소개했었는데, 이 API는 문자열을 GloVe에 의해서 정의된 규칙에 기반한 행렬로 바꿔주는 함수입니다. 이 함수는 문자열을 각각의 토큰으로 분리해주는데, 이 때 토큰이란 간단히 말해 구두점이나 단어, 또는 단어들과 동등한 형태의 작은 문자열을 말합니다. 예를 들어 “Bill traveled to the kitchen,”이라는 문장에는 6개의 토큰들이 있습니다. 그 중 다섯 개는 각 단어로 구성되어 있고, 마지막이 마침표입니다. 각 토큰들은 개별적으로 벡터화되어 있고, 결과적으로 그림 2에서 보여지는 것과 같이 각 문장으로 구성된 벡터의 집합으로 나타납니다.\n",
        "\n",
        "bAbI의 몇몇 작업들 내에서, 시스템은 GloVe 단어 벡터화에 들어있지 않은 단어들과 만날 수 있습니다. 이렇게 신경망 내에서 미지의 단어들을 처리하기 위해서 우리는 그런 단어들에 대해서도 일관된 벡터화 과정을 유지해야 합니다. 가장 흔한 방법은 모든 미지의 토큰들을 <UNK>벡터로 바꾸는 것인데, 이 방법은 항상 효율적이지 않습니다. 대신에 우리는 각 미지의 토큰들에 대해서 새 벡터를 그리기 위해서, 우리는 랜덤 기법을 사용합니다. \n",
        "\n",
        " \n",
        "\n",
        "새로운 미지의 토큰을 접했을 때 우선 기존의 (Gaussian 근사) 분포를 따르는 기존의 GloVe 벡터로부터 새로운 벡터를 만들고, 그걸 다시 GloVe 단어맵에 추가합니다. 분포에 대한 Hyperparameter를 얻기 위해서, Numpy library는 자동으로 분산과 평균을 계산해주는 함수를 가지고 있습니다.\n",
        "\n",
        " \n",
        "\n",
        "fill_unk 는 우리가 언제든 새로운 단어에 대한 벡터를 얻을 수 있게끔 해줍니다."
      ]
    },
    {
      "metadata": {
        "id": "BkWW_6nMXnT-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "root_name = 'drive/Project/snu_fira_2018/'\n",
        "\n",
        "glove_vectors_file = root_name + \"glove.6B/glove.6B.50d.txt\"\n",
        "\n",
        "\n",
        "#Select \"task 5\"\n",
        "train_set_file = \"qa5_three-arg-relations_train.txt\"\n",
        "test_set_file = \"qa5_three-arg-relations_test.txt\"\n",
        "\n",
        "train_set_post_file = root_name + \"tasks_1-20_v1-2/en/\"+train_set_file\n",
        "test_set_post_file = root_name + \"tasks_1-20_v1-2/en/\"+test_set_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HFv-WvnPXnUO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Deserialize GloVe vectors\n",
        "glove_wordmap = {}\n",
        "with open(glove_vectors_file, \"r\", encoding=\"utf8\") as glove:\n",
        "    for line in glove:\n",
        "        name, vector = tuple(line.split(\" \", 1))\n",
        "        glove_wordmap[name] = np.fromstring(vector, sep=\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gMeQLWUwXnUT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wvecs = []\n",
        "for item in glove_wordmap.items():\n",
        "    wvecs.append(item[1])\n",
        "s = np.vstack(wvecs)\n",
        "\n",
        "# Gather the distribution hyperparameters\n",
        "v = np.var(s,0) \n",
        "m = np.mean(s,0) \n",
        "RS = np.random.RandomState()\n",
        "\n",
        "def fill_unk(unk):\n",
        "    global glove_wordmap\n",
        "    glove_wordmap[unk] = RS.multivariate_normal(m,np.diag(v))\n",
        "    return glove_wordmap[unk]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dvbkrwiYXnUX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 알려진 것과 알려지지 않은 것\n",
        "\n",
        " \n",
        "\n",
        "bAbI 작업에 쓰이는 제한된 어휘는 신경망이 굳이 단어가 무슨 뜻을 가지는 알 필요 없이 단어들 사이의 관계를 배울 수 있다는 것을 의미합니다. 하지만 학습 속도를 위해서라도 고유의 의미를 가지는 벡터화 작업을 선택해야 합니다. 이를 위해서 Stanford 대학에서 배포한 GloVe 단어 벡터 데이터 집합 내에 존재하는 단어들에 대한 greedy 탐색을 사용하고, 만약 단어가 존재하지 않을 경우, 우리는 미지의, 랜덤하게 생성되고, 새로운 의미를 나타내는 전체 단어를 채워 넣어야 합니다.\n",
        "\n",
        " \n",
        "\n",
        "단어 벡터 모델을 통해 우리는 새로운 sentence2sequence를 정의할 수 있습니다."
      ]
    },
    {
      "metadata": {
        "id": "Hn5GourpXnUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sentence2sequence(sentence):\n",
        "    \"\"\"\n",
        "     \n",
        "    - Turns an input paragraph into an (m,d) matrix, \n",
        "        where n is the number of tokens in the sentence\n",
        "        and d is the number of dimensions each word vector has.\n",
        "    \n",
        "      TensorFlow doesn't need to be used here, as simply\n",
        "      turning the sentence into a sequence based off our \n",
        "      mapping does not need the computational power that\n",
        "      TensorFlow provides. Normal Python suffices for this task.\n",
        "    \"\"\"\n",
        "    tokens = sentence.strip('\"(),-').lower().split(\" \")\n",
        "    rows = []\n",
        "    words = []\n",
        "    #Greedy search for tokens\n",
        "    for token in tokens:\n",
        "        i = len(token)\n",
        "        while len(token) > 0:\n",
        "            word = token[:i]\n",
        "            if word in glove_wordmap:\n",
        "                rows.append(glove_wordmap[word])\n",
        "                words.append(word)\n",
        "                token = token[i:]\n",
        "                i = len(token)\n",
        "                continue\n",
        "            else:\n",
        "                i = i-1\n",
        "            if i == 0:\n",
        "                # word OOV\n",
        "                # https://arxiv.org/pdf/1611.01436.pdf\n",
        "                rows.append(fill_unk(token))\n",
        "                words.append(token)\n",
        "                break\n",
        "    return np.array(rows), words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XAIgUhRcXnUe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "이제 문맥, 질문, 답변의 벡터들을 포함한, 각 질문에 필요한 데이터들을 하나로 묶을 수 있습니다. bAbI에서 문맥은 몇몇 문장의 연속된 형태로 정의되는데, 이 것들은 contextualize이 여러 개의 문장을 하나의 문맥으로 비직렬화하면서 만들어진 것입니다. 질문과 답변들은 같은 줄 상에 있고, 탭으로 나눠져 있기 때문에 우리는 탭을 이용해서 특정 줄이 질문인지 아닌지를 가리는 용도로 사용할 수 있습니다. 번호 매기기가 재설정되면, 향후에 나올 질문들은 새로운 문맥을 참조하게 됩니다. (참고로 단일 문맥에 상응하는 질문이 하나 이상이 되는 경우가 가끔 있습니다.) 답변은 우리가 가지고 있지만 사용할 필요가 없는 정보도 가지고 있는데, 질문에 대답하는데 필요한 문장에 대한 숫자 같은 게 그런 것입니다. 우리 시스템 상에서는 신경망이 어떤 문장이 질문에 대답하는데 필요한 것인지를 스스로 가르칠 것입니다."
      ]
    },
    {
      "metadata": {
        "id": "pvyBKCEcXnUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def contextualize(set_file):\n",
        "    \"\"\"\n",
        "    Read in the dataset of questions and build question+answer -> context sets.\n",
        "    Output is a list of data points, each of which is a 7-element tuple containing:\n",
        "        The sentences in the context in vectorized form.\n",
        "        The sentences in the context as a list of string tokens.\n",
        "        The question in vectorized form.\n",
        "        The question as a list of string tokens.\n",
        "        The answer in vectorized form.\n",
        "        The answer as a list of string tokens.\n",
        "        A list of numbers for supporting statements, which is currently unused.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    context = []\n",
        "    with open(set_file, \"r\", encoding=\"utf8\") as train:\n",
        "        for line in train:\n",
        "            l, ine = tuple(line.split(\" \", 1))\n",
        "            # Split the line numbers from the sentences they refer to.\n",
        "            if l is \"1\":\n",
        "                # New contexts always start with 1, \n",
        "                # so this is a signal to reset the context.\n",
        "                context = []\n",
        "            if \"\\t\" in ine: \n",
        "                # Tabs are the separator between questions and answers,\n",
        "                # and are not present in context statements.\n",
        "                question, answer, support = tuple(ine.split(\"\\t\"))\n",
        "                data.append((tuple(zip(*context))+\n",
        "                             sentence2sequence(question)+\n",
        "                             sentence2sequence(answer)+\n",
        "                             ([int(s) for s in support.split()],)))\n",
        "                # Multiple questions may refer to the same context, so we don't reset it.\n",
        "            else:\n",
        "                # Context sentence.\n",
        "                context.append(sentence2sequence(ine[:-1]))\n",
        "    return data\n",
        "train_data = contextualize(train_set_post_file)\n",
        "test_data = contextualize(test_set_post_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "CoomRP8CXnUl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "final_train_data = []\n",
        "def finalize(data):\n",
        "    \"\"\"\n",
        "    Prepares data generated by contextualize() for use in the network.\n",
        "    \"\"\"\n",
        "    final_data = []\n",
        "    for cqas in data:\n",
        "        contextvs, contextws, qvs, qws, avs, aws, spt = cqas\n",
        "\n",
        "        lengths = itertools.accumulate(len(cvec) for cvec in contextvs)\n",
        "        context_vec = np.concatenate(contextvs)\n",
        "        context_words = sum(contextws,[])\n",
        "        \n",
        "        # Location markers for the beginnings of new sentences.\n",
        "        sentence_ends = np.array(list(lengths)) \n",
        "        final_data.append((context_vec, sentence_ends, qvs, spt, context_words, cqas, avs, aws))\n",
        "    return np.array(final_data)\n",
        "final_train_data = finalize(train_data)   \n",
        "final_test_data = finalize(test_data)     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r6qv_a4xXnUo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter 정의\n",
        "\n",
        " \n",
        "\n",
        "여기서 우리는 우리가 학습시킬 데이터와 검증을 위한 데이터를 모두 준비했습니다. 다음 작업은 우리가 사용할 신경망이 데이터를 이해할 수 있도록 만드는 작업입니다. TensorFlow 기본 그래프를 치움으로써 우리가 무언가를 변화시켰을 때 신경망을 다시 돌릴 수 있는 옵션을 가지고 있다는 사실부터 시작해봅시다."
      ]
    },
    {
      "metadata": {
        "id": "k4V8uzzpXnUq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "owcnyishXnUt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "사실 이게 진짜 신경망의 서두이기 때문에, 신경망에 필요한 모든 상수들도 정의해봅시다. 우리는 이걸 hyperparameter라고 부르는데, 이 값들은 신경망의 외관과 학습에 대해서 정의합니다."
      ]
    },
    {
      "metadata": {
        "id": "RukYvURCXnUv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "# The number of dimensions used to store data passed between recurrent layers in the network.\n",
        "recurrent_cell_size = 128\n",
        "\n",
        "# The number of dimensions in our word vectorizations.\n",
        "D = 50 \n",
        "\n",
        "# How quickly the network learns. Too high, and we may run into numeric instability \n",
        "# or other issues.\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Dropout probabilities. For a description of dropout and what these probabilities are, \n",
        "# see Entailment with TensorFlow.\n",
        "input_p, output_p = 0.5, 0.5\n",
        "\n",
        "# How many questions we train on at a time.\n",
        "batch_size = 128\n",
        "\n",
        "# Number of passes in episodic memory. We'll get to this later.\n",
        "passes = 4\n",
        "\n",
        "# Feed Forward layer sizes: the number of dimensions used to store data passed from feed-forward layers.\n",
        "ff_hidden_size = 256\n",
        "\n",
        "weight_decay = 0.00000001\n",
        "# The strength of our regularization. Increase to encourage sparsity in episodic memory, \n",
        "# but makes training slower. Don't make this larger than leraning_rate.\n",
        "\n",
        "training_iterations_count = 400000\n",
        "# How many questions the network trains on each time it is trained. \n",
        "# Some questions are counted multiple times.\n",
        "\n",
        "display_step = 100\n",
        "# How many iterations of training occur before each validation check."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NW2Xni2AXnUz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 신경망 구조\n",
        "\n",
        " \n",
        "\n",
        "Hyperparameter에 대해서 끝났으니 이제 신경망 구조에 대해 묘사해보겠습니다.  이 신경망의 구조는 대충 4개의 모듈로 나눠지고 Ask Me Anything: Dynamic Memory Networks for Natural Language Processing 에 묘사되어 있습니다.\n",
        "\n",
        " \n",
        "\n",
        "신경망은 순환(recurrent) 계층의 메모리가 동적으로 설정되게끔 설계되어 있고, 이건 텍스트속의 다른 정보를 기반으로 만들어진건데, 보통 이걸 Dynamic Memory Network(DMN)이라고 합니다. DMN은 보통 인간이 읽고-이해하는 종류의 질문에 대해서 답변할 때 시도하는 방법에 대한 이해에 기반한 것입니다. 우선 사람은 문맥을 읽고, 그 속에서 사실에 대한 기억을 생산할 기회를 잡습니다. 그런 사실들이 마음속에 있고 난 후에 사람들은 질문을 읽고, 그 질문을 각 사실과 비교해보면서 질문에 대한 답변을 세밀하게 찾기 위해 문맥을 재탐색합니다. \n",
        "\n",
        " 때때로 한가지 사실이 다른 것을 유도하기도 합니다. bAbI 데이터 집합군에서 신경망은 축구의 위치를 찾기를 원할 것입니다. John이 축구공을 만진 마지막 사람이었다는 사실을 찾기 위해 축구에 대한 문장을 검색한 다음, John이 침실과 복도에 모두 있었음을 알기 위해 John에 대한 문장을 검색할 수 있습니다.\n",
        " \n",
        " #### Input\n",
        "\n",
        " \n",
        "\n",
        "Input 모듈은 4개의 모듈중 가장 첫번째로, Dynamic Memory Network이 입력에 대한 대답을 내놓기 위한 모듈이고, 증거들을 얻기 위해서 입력에 대한 통로가 GRU라고 불리는 Gated Recurrent Unit과 함께 있는 형태로 구성되어 있습니다. (TensorFlow에서는 tf.contrib.nn.GRUCell에 있습니다.) 사실이라고 언급되는 각각의 증거들은 문맥 속에서 하나의 문장의 형태로 나타나고, 각 시간대별로 출력으로 표현됩니다. 이런 과정은 약간의 TensorFlow 이외의 전처리 과정이 필요하기 때문에 문장의 끝 부분을 모아서 나중에 모듈로 사용하기 위해서 TensorFlow로 전달할 수 있습니다.\n",
        "\n",
        " \n",
        "\n",
        "우리는 학습 과정을 거칠 때 외적으로 필요한 처리 과정에 대해서 다룰 예정입니다. 그리고 관련된 출력을 선택하기 위해서 TensorFlow에서 제공하는 gather_nd 라는 함수를 통해 처리된 데이터를 활용할 수 있습니다. gather_nd 함수는 엄청나게 유용한 함수이며, 한번 API 문서를 통해서 이게 어떻게 동작하는지 확인해보실 것을 추천해드립니다."
      ]
    },
    {
      "metadata": {
        "id": "wx7YLZbIXnU2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Input Module\n",
        "\n",
        "# Context: A [batch_size, maximum_context_length, word_vectorization_dimensions] tensor \n",
        "# that contains all the context information.\n",
        "context = tf.placeholder(tf.float32, [None, None, D], \"context\")  \n",
        "context_placeholder = context # I use context as a variable name later on\n",
        "\n",
        "# input_sentence_endings: A [batch_size, maximum_sentence_count, 2] tensor that \n",
        "# contains the locations of the ends of sentences. \n",
        "input_sentence_endings = tf.placeholder(tf.int32, [None, None, 2], \"sentence\")\n",
        "\n",
        "# recurrent_cell_size: the number of hidden units in recurrent layers.\n",
        "input_gru = tf.contrib.rnn.GRUCell(recurrent_cell_size)\n",
        "\n",
        "# input_p: The probability of maintaining a specific hidden input unit.\n",
        "# Likewise, output_p is the probability of maintaining a specific hidden output unit.\n",
        "gru_drop = tf.contrib.rnn.DropoutWrapper(input_gru, input_p, output_p)\n",
        "\n",
        "# dynamic_rnn also returns the final internal state. We don't need that, and can\n",
        "# ignore the corresponding output (_). \n",
        "input_module_outputs, _ = tf.nn.dynamic_rnn(gru_drop, context, dtype=tf.float32, scope = \"input_module\")\n",
        "\n",
        "# cs: the facts gathered from the context.\n",
        "cs = tf.gather_nd(input_module_outputs, input_sentence_endings)\n",
        "# to use every word as a fact, useful for tasks with one-sentence contexts\n",
        "s = input_module_outputs "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GaHnPbIeXnU6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Question\n",
        "\n",
        " \n",
        "\n",
        "Question 모듈은 두번째 모듈인데, 이론적으로 말하면 가장 간단합니다. 이번에는 질문의 텍스트에 대해서 또다른 GRU 경로로 구성됩니다. 앞에서 다뤘던 증거 대신에 우리는 한 문장으로 길게 설정된 자료에 의해 보장된 질문을 통해서 단순히 최종 상태를 전달할 수 있습니다.\n",
        "\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "m0NbE1_ZXnU8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Question Module\n",
        "\n",
        "# query: A [batch_size, maximum_question_length, word_vectorization_dimensions] tensor \n",
        "#  that contains all of the questions.\n",
        "\n",
        "query = tf.placeholder(tf.float32, [None, None, D], \"query\")\n",
        "\n",
        "# input_query_lengths: A [batch_size, 2] tensor that contains question length information. \n",
        "# input_query_lengths[:,1] has the actual lengths; input_query_lengths[:,0] is a simple range() \n",
        "# so that it plays nice with gather_nd.\n",
        "input_query_lengths = tf.placeholder(tf.int32, [None, 2], \"query_lengths\")\n",
        "\n",
        "question_module_outputs, _ = tf.nn.dynamic_rnn(gru_drop, query, dtype=tf.float32, \n",
        "                                               scope = tf.VariableScope(True, \"input_module\"))\n",
        "\n",
        "# q: the question states. A [batch_size, recurrent_cell_size] tensor.\n",
        "q = tf.gather_nd(question_module_outputs, input_query_lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VwEE06i9XnU-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Episodic memory\n",
        "\n",
        " \n",
        "\n",
        "우리의 세번째 모듈인, Episodic 모듈은 재미있는 모듈입니다. 여기서 여러 경로를 활용하기 위해 attention을 사용하는데, 각 경로 별로 입력을 반복하는 GRU들로 구성되어 있습니다. 각 경로 내에서 반복되는 횟수마다 현재 저장된 공간에 대해서 가중치가 가해진 업데이트가 발생하는데, 이는 해당 시간에 사실에 얼마나 관심을 기울였느냐에 기반합니다.\n",
        "\n",
        " \n",
        "\n",
        "##### Attention\n",
        "\n",
        " \n",
        "\n",
        "신경망상에서 Attention란 본래 이미지 처리에 사용되도록 설계되었는데, 특히 이미지의 특정 부분이 다른 부분보다 더 의미 있는 케이스에서 활용되었습니다. 신경망은 보통 동작을 수행할 때 어떤 부분이 처리시 가장 최적의 위치인지를 찾는데 Attention를 사용합니다. 예를 들어 이미지 내에서 사물의 위치를 찾는다거나 이미지들 사이에서 움직이는 물체를 추적한다던지, 얼굴 인식, 그밖에 이미지 안에서 가장 적절한 정보를 찾는 데에서 이점을 가지는 작업들이 그런 것입니다.\n",
        "\n",
        " \n",
        "\n",
        " 가장 핵심적인 문제는 정확히 하나의 입력과 연관되어 있는 hard attention라도 쉽게 최적화되기 어렵다는 점입니다. 다른 대부분의 신경망과 동일하게 우리의 최적화 과정은 우리의 입력과 가중치에 기반하여 손실함수에 대한 미분계수를 계산하는 것인데, hard attention 자체가 2진법 특성 때문에 쉽게 미분이 되지 않습니다. 대신 soft attention이라고 알려져 있는 실제 값들을 사용하게 되는데, 이는 어떤 형태의 가중치를 사용할 수 있는 모든 입력들을 결합합니다. 고맙게도 가중치를 주는 작업은 완전 미분이 가능하고, 일반적으로 학습이 가능합니다. Hard attention을 학습시킬 수 있어도 이는 매우 힘든 작업이 될 것이고, 때때로는 soft attention에 비해서 나쁘게 동작할 수 있습니다. 그렇기 때문에 우리는 현재 모델에 soft attention을 사용할 것입니다. 미분계수를 코딩하는 것에 대해서 걱정할 것이 없는 게, TensorFlow에서 제공하는 최적화 관련 scheme이 이 작업을 대신 해줍니다.\n",
        "\n",
        " \n",
        "\n",
        " 우리는 이 모델 안에서 각 사실, 현재 저장된 메모리와 원래의 질문들 사이에서 유사성을 만듦으로써 attention을 계산합니다.(참고로 지금 언급하고 있는 것은 사실과 현재 저장된 메모리만 기반해서 생성된 유사성 기반의 일반적인 attention과는 다른 것입니다.) 우리는 각 사실에 대한 attention의 상수를 얻기 위해서 이 결과를 두 개의 계층으로 구성된 feed-forward 신경망에 통과시킵니다. 그러고 난 뒤 (이전 attention 상수로 가중치가 가해진)입력 사실 상으로 가중치가 더해진 GRU로 작업을 함으로써 저장된 메모리를 수정합니다. 문맥이 전체 행렬에 비해서 짧을 때 메모리에 잘못된 정보가 저장되는 것을 막기 위해 우리는 사실일 때와 (같은 메모리를 존속시킨 것과 같이) 사실이 존재하지 않을 때를 구별하기 위한 마스크를 생성합니다.\n",
        "\n",
        " \n",
        "\n",
        " 또 다른 주목할만한 측면은 이렇게 생성된 attention 마스크가 계층에 의해서 사용되는 표시 주변으로 항상 감싸게 된다는 것입니다. 이미지 측면에서 보면 이런 역할은 보통 convolutional 계층 주변에서 발생하며(이미지 내에서 위치를 직접적으로 맵핑하는 것과 같이) 자연언어처리 관점에서는 이런 동작이 recurrent 계층 주변에서 발생한다는 것입니다. Feed-forward 계층 주변에서 attention으로 감싸는 것은 기술적으로 가능하지만 적어도 연이어 나올 feed-forward 계층에 의해서 더 쉽게 시뮬레이션 될 수 없는 현 상황에서는 유용하지는 않습니다.\n"
      ]
    },
    {
      "metadata": {
        "id": "-zgKuYoUXnVA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "e10239a4-ec3a-419c-fc8d-b848aef6eb40"
      },
      "cell_type": "code",
      "source": [
        "# Episodic Memory\n",
        "\n",
        "# make sure the current memory (i.e. the question vector) is broadcasted along the facts dimension\n",
        "size = tf.stack([tf.constant(1),tf.shape(cs)[1], tf.constant(1)])\n",
        "re_q = tf.tile(tf.reshape(q,[-1,1,recurrent_cell_size]),size)\n",
        "\n",
        "\n",
        "# Final output for attention, needs to be 1 in order to create a mask\n",
        "output_size = 1 \n",
        "\n",
        "# Weights and biases\n",
        "attend_init = tf.random_normal_initializer(stddev=0.1)\n",
        "w_1 = tf.get_variable(\"attend_w1\", [1,recurrent_cell_size*7, recurrent_cell_size], \n",
        "                      tf.float32, initializer = attend_init)\n",
        "w_2 = tf.get_variable(\"attend_w2\", [1,recurrent_cell_size, output_size], \n",
        "                      tf.float32, initializer = attend_init)\n",
        "\n",
        "b_1 = tf.get_variable(\"attend_b1\", [1, recurrent_cell_size], \n",
        "                      tf.float32, initializer = attend_init)\n",
        "b_2 = tf.get_variable(\"attend_b2\", [1, output_size], \n",
        "                      tf.float32, initializer = attend_init)\n",
        "\n",
        "# Regulate all the weights and biases\n",
        "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(w_1))\n",
        "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(b_1))\n",
        "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(w_2))\n",
        "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(b_2))\n",
        "\n",
        "def attention(c, mem, existing_facts):\n",
        "    \"\"\"\n",
        "    Custom attention mechanism.\n",
        "    c: A [batch_size, maximum_sentence_count, recurrent_cell_size] tensor \n",
        "        that contains all the facts from the contexts.\n",
        "    mem: A [batch_size, maximum_sentence_count, recurrent_cell_size] tensor that \n",
        "        contains the current memory. It should be the same memory for all facts for accurate results.\n",
        "    existing_facts: A [batch_size, maximum_sentence_count, 1] tensor that \n",
        "        acts as a binary mask for which facts exist and which do not.\n",
        "    \n",
        "    \"\"\"\n",
        "    with tf.variable_scope(\"attending\") as scope:\n",
        "        # attending: The metrics by which we decide what to attend to.\n",
        "        attending = tf.concat([c, mem, re_q, c * re_q,  c * mem, (c-re_q)**2, (c-mem)**2], 2)\n",
        "        \n",
        "        # m1: First layer of multiplied weights for the feed-forward network. \n",
        "        #     We tile the weights in order to manually broadcast, since tf.matmul does not\n",
        "        #     automatically broadcast batch matrix multiplication as of TensorFlow 1.2.\n",
        "        m1 = tf.matmul(attending * existing_facts, \n",
        "                       tf.tile(w_1, tf.stack([tf.shape(attending)[0],1,1]))) * existing_facts\n",
        "        # bias_1: A masked version of the first feed-forward layer's bias\n",
        "        #     over only existing facts.\n",
        "\n",
        "        bias_1 = b_1 * existing_facts\n",
        "        \n",
        "        # tnhan: First nonlinearity. In the original paper, this is a tanh nonlinearity; \n",
        "        #        choosing relu was a design choice intended to avoid issues with \n",
        "        #        low gradient magnitude when the tanh returned values close to 1 or -1. \n",
        "        tnhan = tf.nn.relu(m1 + bias_1)\n",
        "        \n",
        "        # m2: Second layer of multiplied weights for the feed-forward network. \n",
        "        #     Still tiling weights for the same reason described in m1's comments.\n",
        "        m2 = tf.matmul(tnhan, tf.tile(w_2, tf.stack([tf.shape(attending)[0],1,1])))\n",
        "        \n",
        "        # bias_2: A masked version of the second feed-forward layer's bias.\n",
        "        bias_2 = b_2 * existing_facts\n",
        "        \n",
        "        # norm_m2: A normalized version of the second layer of weights, which is used \n",
        "        #     to help make sure the softmax nonlinearity doesn't saturate.\n",
        "        norm_m2 = tf.nn.l2_normalize(m2 + bias_2, -1)\n",
        "        \n",
        "        # softmaxable: A hack in order to use sparse_softmax on an otherwise dense tensor. \n",
        "        #     We make norm_m2 a sparse tensor, then make it dense again after the operation.\n",
        "        softmax_idx = tf.where(tf.not_equal(norm_m2, 0))[:,:-1]\n",
        "        softmax_gather = tf.gather_nd(norm_m2[...,0], softmax_idx)\n",
        "        softmax_shape = tf.shape(norm_m2, out_type=tf.int64)[:-1]\n",
        "        softmaxable = tf.SparseTensor(softmax_idx, softmax_gather, softmax_shape)\n",
        "        return tf.expand_dims(tf.sparse_tensor_to_dense(tf.sparse_softmax(softmaxable)),-1)\n",
        "\n",
        "# facts_0s: a [batch_size, max_facts_length, 1] tensor \n",
        "#     whose values are 1 if the corresponding fact exists and 0 if not.\n",
        "facts_0s = tf.cast(tf.count_nonzero(input_sentence_endings[:,:,-1:],-1,keep_dims=True),tf.float32)\n",
        "\n",
        "\n",
        "with tf.variable_scope(\"Episodes\") as scope:\n",
        "    attention_gru = tf.contrib.rnn.GRUCell(recurrent_cell_size)\n",
        "    \n",
        "    # memory: A list of all tensors that are the (current or past) memory state \n",
        "    #   of the attention mechanism.\n",
        "    memory = [q]\n",
        "    \n",
        "    # attends: A list of all tensors that represent what the network attends to.\n",
        "    attends = []\n",
        "    for a in range(passes):\n",
        "        # attention mask\n",
        "        attend_to = attention(cs, tf.tile(tf.reshape(memory[-1],[-1,1,recurrent_cell_size]),size),\n",
        "                              facts_0s)\n",
        "        \n",
        "        # Inverse attention mask, for what's retained in the state.\n",
        "        retain = 1-attend_to\n",
        "        \n",
        "        # GRU pass over the facts, according to the attention mask.\n",
        "        while_valid_index = (lambda state, index: index < tf.shape(cs)[1])\n",
        "        update_state = (lambda state, index: (attend_to[:,index,:] * \n",
        "                                                 attention_gru(cs[:,index,:], state)[0] + \n",
        "                                                 retain[:,index,:] * state))\n",
        "        # start loop with most recent memory and at the first index\n",
        "        memory.append(tuple(tf.while_loop(while_valid_index,\n",
        "                          (lambda state, index: (update_state(state,index),index+1)),\n",
        "                           loop_vars = [memory[-1], 0]))[0]) \n",
        "        \n",
        "        attends.append(attend_to)\n",
        "        \n",
        "        # Reuse variables so the GRU pass uses the same variables every pass.\n",
        "        scope.reuse_variables()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-757421b0191e>:77: calling count_nonzero (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ayTPFoEjXnVG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Answer\n",
        "\n",
        " \n",
        "\n",
        "마지막 모듈은 Answer 모듈인데, fully connected 계층을 사용해서 question 모듈과 episodic memory 모듈에서 얻은 결과를 “최종 결과”인 단어 벡터 형태로 회귀해주는 것인데, 그 결과와 가장 가까운 문맥에서의 단어로 표현해줍니다. (실제 단어 형태로 보장해줍니다.) 우리는 각 단어별로 “점수”를 계산함으로써 가장 가까운 단어를 파악하는데, 이 점수는 결국 단어와 최종 결과간의 거리를 나타냅니다. 만일 당신이 여러 개의 단어를 반환할 수 있는 answer 모듈을 설계하는 것이라면, 우리가 이 글에서 다루고 있는 bAbI 동작은 필요하지 않습니다."
      ]
    },
    {
      "metadata": {
        "id": "_RksvEp3XnVJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Answer Module\n",
        "\n",
        "# a0: Final memory state. (Input to answer module)\n",
        "a0 = tf.concat([memory[-1], q], -1)\n",
        "\n",
        "# fc_init: Initializer for the final fully connected layer's weights.\n",
        "fc_init = tf.random_normal_initializer(stddev=0.1) \n",
        "\n",
        "with tf.variable_scope(\"answer\"):\n",
        "    # w_answer: The final fully connected layer's weights.\n",
        "    w_answer = tf.get_variable(\"weight\", [recurrent_cell_size*2, D], \n",
        "                               tf.float32, initializer = fc_init)\n",
        "    # Regulate the fully connected layer's weights\n",
        "    tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, \n",
        "                     tf.nn.l2_loss(w_answer)) \n",
        "    \n",
        "    # The regressed word. This isn't an actual word yet; \n",
        "    #    we still have to find the closest match.\n",
        "    logit = tf.expand_dims(tf.matmul(a0, w_answer),1)\n",
        "    \n",
        "    # Make a mask over which words exist.\n",
        "    with tf.variable_scope(\"ending\"):\n",
        "        all_ends = tf.reshape(input_sentence_endings, [-1,2])\n",
        "        range_ends = tf.range(tf.shape(all_ends)[0])\n",
        "        ends_indices = tf.stack([all_ends[:,0],range_ends], axis=1)\n",
        "        ind = tf.reduce_max(tf.scatter_nd(ends_indices, all_ends[:,1],\n",
        "                                          [tf.shape(q)[0], tf.shape(all_ends)[0]]),\n",
        "                            axis=-1)\n",
        "        range_ind = tf.range(tf.shape(ind)[0])\n",
        "        mask_ends = tf.cast(tf.scatter_nd(tf.stack([ind, range_ind], axis=1), \n",
        "                                          tf.ones_like(range_ind), [tf.reduce_max(ind)+1, \n",
        "                                                                    tf.shape(ind)[0]]), bool)\n",
        "        # A bit of a trick. With the locations of the ends of the mask (the last periods in \n",
        "        #  each of the contexts) as 1 and the rest as 0, we can scan with exclusive or \n",
        "        #  (starting from all 1). For each context in the batch, this will result in 1s \n",
        "        #  up until the marker (the location of that last period) and 0s afterwards.\n",
        "        mask = tf.scan(tf.logical_xor,mask_ends, tf.ones_like(range_ind, dtype=bool))\n",
        "        \n",
        "    # We score each possible word inversely with their Euclidean distance to the regressed word.\n",
        "    #  The highest score (lowest distance) will correspond to the selected word.\n",
        "    logits = -tf.reduce_sum(tf.square(context*tf.transpose(tf.expand_dims(\n",
        "                    tf.cast(mask, tf.float32),-1),[1,0,2]) - logit), axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SnJXoEHJXnVN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 최적화 과정\n",
        "\n",
        " \n",
        "\n",
        "경사 하강법(Gradient Descent)은 신경망에서 흔하게 쓰이는 최적화 도구입니다. 이 도구의 목적은 신경망의 “손실”을 줄이는 것인데, 이 손실은 신경망이 얼마나 나쁘게 동작하는지를 나타내는 척도입니다. 이 방법은 현재 입력 상에서 각 가중치에 관하여 손실에 대한 미분계수를 찾은 후, 가중치를 “하강”시킴으로써 손실을 줄일 수 있도록 되어 있습니다. 대부분이 이 방법은 충분히 잘 동작했지만 이상적이지 않습니다. “모멘텀”이나 이상적인 가중치에 대한 직관적인 계산을 구하는데 쓰이는 근사법을 사용하는 여러가지 scheme들이 있는데 그 중 가장 유용한 방법 중 하나가 적응 모멘트 추정법(Adaptive Moment estimation), 또는 Adam이라고 알려진 것입니다.\n",
        "\n",
        " \n",
        "\n",
        "Adam은 과거의 반복된 경사값과 해당 값의 제곱된 경사의 기하 급수적 평균을 계산함으로써 처음 두 구간 사이의 경사를 추정합니다. 이 때 구해진 평균은 이런 경사로부터 추정된 평균과 추정된 분산에 해당합니다. 계산시 새로운 정보를 추가했을 때 평균이 얼마나 빨리 감소하는지를 나타내기 위해 두 개의 추가 매개변수를 사용합니다. 평균은 0으로 초기화되는데, 특히 0 부근의 hyperparameter의 경우 0으로 편향되는 경향이 있습니다.\n",
        "\n",
        " \n",
        "\n",
        "이런 편향을 막기 위해서, Adam은 원본보다 더 큰 강도를 가진 편향이 수정된 모멘트 근사치를 계산합니다. 이렇게 수정된 근사치는 신경망 내에서 가중치를 업데이트하는데 사용됩니다. 이런 근사치 조합들이 최적화 과정 전반적으로 보았을 때 Adam을 가장 좋은 것으로 만드는데, 특히 복잡한 신경망에서 빛을 발합니다. Adam은 자연언어처리 작업에서 흔히 발생하는 것과 같이 매우 희소한 데이터에 대해서는 두 번 적용됩니다.\n",
        "\n",
        " \n",
        "\n",
        "TensorFlow에서는 tf.train.AdamOptimizer를 통해 Adam을 사용할 수 있습니다.`tf.train.AdamOptimizer`."
      ]
    },
    {
      "metadata": {
        "id": "BKqvzIoEXnVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "35f4b56b-2398-40da-f888-66444b046a03"
      },
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "# gold_standard: The real answers.\n",
        "gold_standard = tf.placeholder(tf.float32, [None, 1, D], \"answer\")\n",
        "with tf.variable_scope('accuracy'):\n",
        "    eq = tf.equal(context, gold_standard)\n",
        "    corrbool = tf.reduce_all(eq,-1)\n",
        "    logloc = tf.reduce_max(logits, -1, keep_dims = True)\n",
        "    # locs: A boolean tensor that indicates where the score \n",
        "    #  matches the minimum score. This happens on multiple dimensions, \n",
        "    #  so in the off chance there's one or two indexes that match \n",
        "    #  we make sure it matches in all indexes.\n",
        "    locs = tf.equal(logits, logloc)\n",
        "    \n",
        "    # correctsbool: A boolean tensor that indicates for which \n",
        "    #   words in the context the score always matches the minimum score.\n",
        "    correctsbool = tf.reduce_any(tf.logical_and(locs, corrbool), -1)\n",
        "    # corrects: A tensor that is simply correctsbool cast to floats.\n",
        "    corrects = tf.where(correctsbool, tf.ones_like(correctsbool, dtype=tf.float32), \n",
        "                        tf.zeros_like(correctsbool,dtype=tf.float32))\n",
        "    \n",
        "    # corr: corrects, but for the right answer instead of our selected answer.\n",
        "    corr = tf.where(corrbool, tf.ones_like(corrbool, dtype=tf.float32), \n",
        "                        tf.zeros_like(corrbool,dtype=tf.float32))\n",
        "with tf.variable_scope(\"loss\"):\n",
        "    # Use sigmoid cross entropy as the base loss, \n",
        "    #  with our distances as the relative probabilities. There are\n",
        "    #  multiple correct labels, for each location of the answer word within the context.\n",
        "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits = tf.nn.l2_normalize(logits,-1),\n",
        "                                                   labels = corr)\n",
        "    \n",
        "    # Add regularization losses, weighted by weight_decay.\n",
        "    total_loss = tf.reduce_mean(loss) + weight_decay * tf.add_n(\n",
        "        tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
        "\n",
        "# TensorFlow's default implementation of the Adam optimizer works. We can adjust more than \n",
        "#  just the learning rate, but it's not necessary to find a very good optimum.\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "# Once we have an optimizer, we ask it to minimize the loss \n",
        "#   in order to work towards the proper training.\n",
        "opt_op = optimizer.minimize(total_loss)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-19-a6881492e1cf>:5: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gsDuVg5cXnVR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the TensorFlow session\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "t7gzDTimXnVW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 신경망 학습\n",
        "\n",
        " \n",
        "\n",
        "모든 것이 준비되었을 때, 우리는 우리가 만든 신경망을 학습시키기 위해서 데이터를 일괄 처리할 수 있습니다. 시스템이 학습하는 동안, 우리는 정확성 관점에서 이 신경망이 잘 동작하는지를 확인해야 합니다. 이 작업을 검증군으로 할 수 있는데, 이 검증군은 테스트 데이터로부터 빼기 때문에 학습 데이터와 겹치는 부분이 없습니다.\n",
        "\n",
        " \n",
        "\n",
        "테스트 데이터 기반의 검증군을 활용하게 되면 이 신경망이 학습한 내용을 일반화하고 다른 상황에 얼마나 잘 적용할 수 있는지에 대해서 이해를 할 수 있습니다. 만약 우리가 학습 데이터로 검증을 할 경우, 신경망은 overfit될 것입니다. 다른 말로는 특정 예제에 대해서 배우고, 그것에 대한 대답을 기억하게 함으로써 신경망이 새로운 질문에 대해서 답하는데 전혀 도움이 안되는 현상을 말합니다.\n",
        "\n",
        " \n",
        "\n",
        "만약 TQDM을 설치했다면, 이 것을 활용해서 신경망이 학습하는데 얼마나 걸리고, 학습이 완료되었을 때 근사치를 얻을 때까지의 시간에 대해서 추적할 수 있습니다. 당신은 결과가 충분히 좋다고 느낀다면 Jupyter Notebook이 도는 Kernel에 Interrupt를 검으로써 학습을 언제든 중단시킬 수 있습니다.\n",
        "\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "MD02Vb0wXnVY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prep_batch(batch_data, more_data = False):\n",
        "    \"\"\"\n",
        "        Prepare all the preproccessing that needs to be done on a batch-by-batch basis.\n",
        "    \"\"\"\n",
        "    context_vec, sentence_ends, questionvs, spt, context_words, cqas, answervs, _ = zip(*batch_data)\n",
        "    ends = list(sentence_ends)\n",
        "    maxend = max(map(len, ends))\n",
        "    aends = np.zeros((len(ends), maxend))\n",
        "    for index, i in enumerate(ends):\n",
        "        for indexj, x in enumerate(i):\n",
        "            aends[index, indexj] = x-1\n",
        "    new_ends = np.zeros(aends.shape+(2,))\n",
        "\n",
        "    for index, x in np.ndenumerate(aends):\n",
        "        new_ends[index+(0,)] = index[0]\n",
        "        new_ends[index+(1,)] = x\n",
        "\n",
        "    contexts = list(context_vec)\n",
        "    max_context_length = max([len(x) for x in contexts])\n",
        "    contextsize = list(np.array(contexts[0]).shape)\n",
        "    contextsize[0] = max_context_length\n",
        "    final_contexts = np.zeros([len(contexts)]+contextsize)\n",
        "\n",
        "    contexts = [np.array(x) for x in contexts]\n",
        "    for i, context in enumerate(contexts):\n",
        "        final_contexts[i,0:len(context),:] = context\n",
        "    max_query_length = max(len(x) for x in questionvs)\n",
        "    querysize = list(np.array(questionvs[0]).shape)\n",
        "    querysize[:1] = [len(questionvs),max_query_length]\n",
        "    queries = np.zeros(querysize)\n",
        "    querylengths = np.array(list(zip(range(len(questionvs)),[len(q)-1 for q in questionvs])))\n",
        "    questions = [np.array(q) for q in questionvs]\n",
        "    for i, question in enumerate(questions):\n",
        "        queries[i,0:len(question),:] = question\n",
        "    data = {context_placeholder: final_contexts, input_sentence_endings: new_ends, \n",
        "                            query:queries, input_query_lengths:querylengths, gold_standard: answervs}\n",
        "    return (data, context_words, cqas) if more_data else data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "Z0_JcktOXnVb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "2f268e7f-3de0-4815-c466-0a919b509faa"
      },
      "cell_type": "code",
      "source": [
        "# Use TQDM if installed\n",
        "tqdm_installed = False\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "    tqdm_installed = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "# Prepare validation set\n",
        "batch = np.random.randint(final_test_data.shape[0], size=batch_size*10)\n",
        "batch_data = final_test_data[batch]\n",
        "\n",
        "validation_set, val_context_words, val_cqas = prep_batch(batch_data, True)\n",
        "\n",
        "# training_iterations_count: The number of data pieces to train on in total\n",
        "# batch_size: The number of data pieces per batch\n",
        "def train(iterations, batch_size):\n",
        "    training_iterations = range(0,iterations,batch_size)\n",
        "    if tqdm_installed:\n",
        "        # Add a progress bar if TQDM is installed\n",
        "        training_iterations = tqdm(training_iterations)\n",
        "\n",
        "    wordz = []\n",
        "    for j in training_iterations:\n",
        "\n",
        "        batch = np.random.randint(final_train_data.shape[0], size=batch_size)\n",
        "        batch_data = final_train_data[batch]\n",
        "\n",
        "        sess.run([opt_op], feed_dict=prep_batch(batch_data))\n",
        "        if (j/batch_size) % display_step == 0:\n",
        "\n",
        "            # Calculate batch accuracy\n",
        "            acc, ccs, tmp_loss, log, con, cor, loc  = sess.run([corrects, cs, total_loss, logit,\n",
        "                                                                context_placeholder,corr, locs], \n",
        "                                                               feed_dict=validation_set)\n",
        "            # Display results\n",
        "            print(\"Iter \" + str(j/batch_size) + \", Minibatch Loss= \",tmp_loss,\n",
        "                  \"Accuracy= \", np.mean(acc))\n",
        "train(30000,batch_size) # Small amount of training for preliminary results"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/235 [00:06<26:54,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 0.0, Minibatch Loss=  0.67305464 Accuracy=  0.0265625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 101/235 [03:01<04:59,  2.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 100.0, Minibatch Loss=  0.6728112 Accuracy=  0.4171875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 201/235 [05:45<01:03,  1.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 200.0, Minibatch Loss=  0.6727884 Accuracy=  0.47421876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235/235 [06:39<00:00,  1.45s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "bB6npqULXnVk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "학습이 조금 수행된 후에, 내부를 살펴보고 신경망으로부터 어떤 답변을 얻을 수 있는지를 봅시다. 아래의 다이어그램을 보면 각 에피소드(행)에 대한 모든 문장(열)별로 attention을 시각화하고 있습니다. 짙은 색은 해당 에피소드에서 특정 문장에 대해 attention을 더 가했다는 것을 나타냅니다.\n",
        "\n",
        " \n",
        "\n",
        "아마 당신은 각 질문 별로 적어도 두 개의 에피소드 사이에서 attention의 차이를 살펴볼 수 있을 겁니다. 하지만 때때로 attention은 하나의 에피소드만으로 답변을 찾을 수도 있고, 어쩌면 네 개의 에피소드가 전부 필요할 수도 있습니다. 만약 attention이 빈칸으로 남아있다면, 아마 해당 케이스는 포화 상태에 빠진 상태이고, 모든 에피소드에 대해서 즉시 관심을 기울일 수 있다는 것을 의미할 것입니다. 이 경우에는, 이런 현상을 막기 위해서 weight_decay를 높게 가지고 학습시킬 수 있습니다. 학습을 거치고 난 후에는, 포화 현상이 매우 흔하게 발생합니다."
      ]
    },
    {
      "metadata": {
        "id": "zMKM5-iOXnVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1347
        },
        "outputId": "4fd4c16f-9965-4cc8-e2bf-42544459ba0b"
      },
      "cell_type": "code",
      "source": [
        "ancr = sess.run([corrbool,locs, total_loss, logits, facts_0s, w_1]+attends+\n",
        "                [query, cs, question_module_outputs],feed_dict=validation_set)\n",
        "a = ancr[0]\n",
        "n = ancr[1]\n",
        "cr = ancr[2]\n",
        "attenders = np.array(ancr[6:-3]) \n",
        "faq = np.sum(ancr[4], axis=(-1,-2)) # Number of facts in each context\n",
        "\n",
        "limit = 5\n",
        "for question in range(min(limit, batch_size)):\n",
        "    plt.yticks(range(passes,0,-1))\n",
        "    plt.ylabel(\"Episode\")\n",
        "    plt.xlabel(\"Question \"+str(question+1))\n",
        "    pltdata = attenders[:,question,:int(faq[question]),0] \n",
        "    # Display only information about facts that actually exist, all others are 0\n",
        "    pltdata = (pltdata - pltdata.mean()) / ((pltdata.max() - pltdata.min() + 0.001)) * 256\n",
        "    plt.pcolor(pltdata, cmap=plt.cm.BuGn, alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "print(list(map((lambda x: x.shape),ancr[3:])), new_ends.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8uwiAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f82e5eebf98>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f82e06e5518>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqUKGuyRVyHCXpAoZ7pJUIcNdkir0/90SkZPpaO92AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f82e065ac18>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f82e063a4e0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f82e059b358>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "80ETzVQlXnVr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "위에 나온 질문에 대해서 답변을 찾기 위해서 우리는 위치로써 문맥 속의 거리 점수에 대한 위치를 활용할 수 있고, 해당 위치에 어떤 단어가 있는지를 확인할 수 있습니다."
      ]
    },
    {
      "metadata": {
        "id": "iF1jOe7rXnVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "4e66e483-ee18-4ced-9fe6-f8a43b91587a"
      },
      "cell_type": "code",
      "source": [
        "# Locations of responses within contexts\n",
        "indices = np.argmax(n,axis=1)\n",
        "\n",
        "# Locations of actual answers within contexts \n",
        "indicesc = np.argmax(a,axis=1)\n",
        "\n",
        "for i,e,cw, cqa in list(zip(indices, indicesc, val_context_words, val_cqas))[:limit]:\n",
        "    ccc = \" \".join(cw)\n",
        "    print(\"TEXT: \",ccc)\n",
        "    print (\"QUESTION: \", \" \".join(cqa[3]))\n",
        "    print (\"RESPONSE: \", cw[i], [\"Correct\", \"Incorrect\"][i!=e])\n",
        "    print(\"EXPECTED: \", cw[e])\n",
        "    print()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEXT:  bill grabbed the football there . bill got the milk there . bill discarded the milk . fred went back to the office . bill took the milk there . bill dropped the milk there . jeff went back to the hallway . jeff went back to the bathroom . mary went to the hallway . bill put down the football . bill journeyed to the bedroom . jeff travelled to the office . mary moved to the garden . mary went back to the kitchen . bill went to the hallway . jeff journeyed to the kitchen . mary got the apple there . mary passed the apple to jeff .\n",
            "QUESTION:  who received the apple ?\n",
            "RESPONSE:  jeff Correct\n",
            "EXPECTED:  jeff\n",
            "\n",
            "TEXT:  bill went back to the kitchen . bill took the apple there . bill passed the apple to mary . mary handed the apple to fred . fred gave the apple to mary . mary journeyed to the office . mary handed the apple to jeff . mary got the football there . jeff left the apple . mary gave the football to jeff .\n",
            "QUESTION:  who did mary give the apple to ?\n",
            "RESPONSE:  fred Incorrect\n",
            "EXPECTED:  jeff\n",
            "\n",
            "TEXT:  mary grabbed the football there . jeff moved to the garden . mary moved to the garden . bill went to the bathroom . bill picked up the apple there . jeff went to the office . fred travelled to the garden . fred journeyed to the bedroom . mary dropped the football there . jeff went to the bathroom . mary journeyed to the office . bill left the apple . fred went back to the office . jeff moved to the bedroom . bill travelled to the office . jeff travelled to the hallway . bill went to the bedroom . mary journeyed to the bedroom . jeff travelled to the kitchen . bill journeyed to the kitchen . jeff went to the bedroom . jeff travelled to the kitchen . fred went back to the bathroom . bill moved to the bathroom . fred grabbed the apple there . fred handed the apple to bill . bill gave the apple to fred . fred passed the apple to bill . mary went back to the bathroom . jeff went to the office . bill passed the apple to mary . mary discarded the apple . fred moved to the hallway . bill picked up the apple there .\n",
            "QUESTION:  who gave the apple to mary ?\n",
            "RESPONSE:  jeff Incorrect\n",
            "EXPECTED:  bill\n",
            "\n",
            "TEXT:  fred travelled to the kitchen . bill travelled to the office . mary travelled to the office . jeff went back to the kitchen . bill travelled to the hallway . fred went to the bathroom . mary travelled to the hallway . mary travelled to the bedroom . fred journeyed to the hallway . mary moved to the bathroom . bill went back to the kitchen . bill went to the hallway . mary journeyed to the kitchen . mary went to the bathroom . jeff journeyed to the office . fred travelled to the garden . fred grabbed the football there . fred discarded the football . bill travelled to the kitchen . jeff went to the garden . jeff travelled to the bedroom . jeff moved to the garden . jeff grabbed the football there . jeff went back to the bedroom . jeff journeyed to the office . bill travelled to the hallway . jeff left the football . bill went to the bedroom . jeff got the milk there . mary went to the kitchen . bill went to the kitchen . jeff journeyed to the bathroom . bill went to the hallway . jeff dropped the milk . mary went to the hallway . mary went to the kitchen . fred travelled to the office . fred went back to the hallway . jeff got the milk there . fred travelled to the office . bill went to the garden . mary journeyed to the bathroom . fred went back to the bathroom . fred journeyed to the office . jeff passed the milk to mary . fred grabbed the football there .\n",
            "QUESTION:  what did jeff give to mary ?\n",
            "RESPONSE:  football Incorrect\n",
            "EXPECTED:  milk\n",
            "\n",
            "TEXT:  jeff grabbed the football there . mary journeyed to the hallway . fred went to the kitchen . jeff handed the football to fred . jeff moved to the hallway . bill went to the bathroom .\n",
            "QUESTION:  what did jeff give to fred ?\n",
            "RESPONSE:  football Correct\n",
            "EXPECTED:  football\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bw8mdIdkXnVv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "이제 계속 학습시켜봅시다. 좋은 결과를 얻기 위해서 긴 시간동안 학습시켜야 할겁니다. (저의 집에 있는 컴퓨터로는 12시간이 걸렸습니다.) 하지만 결과적으로 매우 높은 정확성을 얻게 될 것입니다(90%가 넘는 값으로 말입니다.)\n",
        "\n",
        " \n",
        "\n",
        " Jupyter Notebook이 익숙한 사람은 알겠지만 언제든 학습을 중지시킬 수도 있고, 같은 tf.Session을 유지하는 한 만들어진 신경망을 계속 다룰 수 있습니다. 이 방법은 신경망에서 현재 주고 있는 attention과 대답을 시각화하는데 유용한 방법입니다. "
      ]
    },
    {
      "metadata": {
        "id": "u4sfWdnsXnVx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "outputId": "8b48c4f1-a65d-4d73-d521-5e8898de4123"
      },
      "cell_type": "code",
      "source": [
        "train(training_iterations_count, batch_size)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/3125 [00:03<2:47:13,  3.21s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 0.0, Minibatch Loss=  0.6727886 Accuracy=  0.43203124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 101/3125 [02:50<1:35:16,  1.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 100.0, Minibatch Loss=  0.6727726 Accuracy=  0.5101563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|▋         | 201/3125 [05:40<1:47:48,  2.21s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 200.0, Minibatch Loss=  0.6727657 Accuracy=  0.52265626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 10%|▉         | 301/3125 [08:25<1:30:38,  1.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 300.0, Minibatch Loss=  0.6727603 Accuracy=  0.5726563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 401/3125 [11:10<1:38:44,  2.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 400.0, Minibatch Loss=  0.6727614 Accuracy=  0.57890624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 501/3125 [13:51<1:33:48,  2.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 500.0, Minibatch Loss=  0.6727522 Accuracy=  0.6015625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 601/3125 [16:34<1:29:13,  2.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 600.0, Minibatch Loss=  0.67276555 Accuracy=  0.6015625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 701/3125 [19:21<1:24:50,  2.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 700.0, Minibatch Loss=  0.6727522 Accuracy=  0.63203126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 801/3125 [22:06<1:29:55,  2.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 800.0, Minibatch Loss=  0.67275476 Accuracy=  0.628125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 901/3125 [24:53<1:22:57,  2.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 900.0, Minibatch Loss=  0.6727523 Accuracy=  0.625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 1001/3125 [27:39<1:15:36,  2.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 1000.0, Minibatch Loss=  0.67274696 Accuracy=  0.63984376\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 1101/3125 [30:20<1:07:28,  2.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 1100.0, Minibatch Loss=  0.67275417 Accuracy=  0.6507813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 1201/3125 [33:11<1:15:02,  2.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 1200.0, Minibatch Loss=  0.67274994 Accuracy=  0.6546875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1301/3125 [35:58<1:08:31,  2.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 1300.0, Minibatch Loss=  0.6727504 Accuracy=  0.6507813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|████▍     | 1401/3125 [38:45<1:02:04,  2.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 1400.0, Minibatch Loss=  0.6727599 Accuracy=  0.6304687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 1501/3125 [41:37<55:41,  2.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 1500.0, Minibatch Loss=  0.6727494 Accuracy=  0.6640625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 51%|█████     | 1601/3125 [44:27<51:12,  2.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 1600.0, Minibatch Loss=  0.67275214 Accuracy=  0.653125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 1701/3125 [47:16<48:42,  2.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 1700.0, Minibatch Loss=  0.6727488 Accuracy=  0.671875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 1801/3125 [49:56<42:39,  1.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 1800.0, Minibatch Loss=  0.6727528 Accuracy=  0.65234375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 61%|██████    | 1901/3125 [52:37<41:52,  2.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 1900.0, Minibatch Loss=  0.67276484 Accuracy=  0.6304687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 2001/3125 [55:26<43:07,  2.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 2000.0, Minibatch Loss=  0.6727515 Accuracy=  0.66875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 2101/3125 [58:04<33:22,  1.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 2100.0, Minibatch Loss=  0.672746 Accuracy=  0.675\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|███████   | 2201/3125 [1:00:38<29:18,  1.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 2200.0, Minibatch Loss=  0.672761 Accuracy=  0.6453125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 74%|███████▎  | 2301/3125 [1:03:21<25:32,  1.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 2300.0, Minibatch Loss=  0.67274517 Accuracy=  0.6703125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 2401/3125 [1:06:01<26:59,  2.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 2400.0, Minibatch Loss=  0.67275035 Accuracy=  0.66328126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|████████  | 2501/3125 [1:08:38<21:15,  2.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 2500.0, Minibatch Loss=  0.6727542 Accuracy=  0.6640625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 2601/3125 [1:11:17<17:06,  1.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 2600.0, Minibatch Loss=  0.6727489 Accuracy=  0.6703125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86%|████████▋ | 2701/3125 [1:13:58<14:52,  2.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 2700.0, Minibatch Loss=  0.67275184 Accuracy=  0.66015625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 90%|████████▉ | 2801/3125 [1:16:31<09:37,  1.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 2800.0, Minibatch Loss=  0.67276883 Accuracy=  0.6351563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 2901/3125 [1:19:13<07:03,  1.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 2900.0, Minibatch Loss=  0.6727612 Accuracy=  0.6421875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 3001/3125 [1:21:49<04:28,  2.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 3000.0, Minibatch Loss=  0.67274827 Accuracy=  0.66875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 3101/3125 [1:24:21<00:49,  2.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter 3100.0, Minibatch Loss=  0.67274684 Accuracy=  0.675\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [1:25:00<00:00,  1.56s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9huWf_eShJaz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G5Uz9eRuXnV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "97e925df-d8f5-40c5-c758-f596ac7debe2"
      },
      "cell_type": "code",
      "source": [
        "# Final testing accuracy\n",
        "print(np.mean(sess.run([corrects], feed_dict= prep_batch(final_test_data))[0]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "486EPOmxXnV9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "우리 모델이 얻은 결과를 다 확인했으면 시스템 자원을 복원하기 위해 session을 닫을 수 있습니다. "
      ]
    },
    {
      "metadata": {
        "id": "TEbaXD-3XnV-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MARlVR3DXnWD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 여전히 해볼 것과 실험해봐야 할 내용이 많이 있습니다.\n",
        "\n",
        " \n",
        "\n",
        "* bAbI내의 다른 작업들: 우리는 bAbI가 제공하는 많은 작업들을 단순히 샘플링했습니다. 한번 다른 작업에 맞게끔 전처리하는 과정을 바꾸고 우리가 만든 Dynamic Memory Network이 어떻게 동작하는지 확인해봅시다. 물론 새로운 작업을 돌리기 전에 신경망을 재설정하기를 원할 것입니다. 만약 해당 작업이 문맥 속의 답을 보장하지 않는다면 아마 나온 결과를 사전의 단어와 이와 연관된 벡터들 대신에 비교해보고 싶을 겁니다.(이 작업은 6-10 그리고 17-20에 있습니다) 저는 작업 1과 3을 해볼 것을 추천하는데, 이는 test_set_file과 train_set_file의 값을 변경함으로써 해볼 수 있습니다.\n",
        "\n",
        "* 지도학습: 우리의 attention 메커니즘은 비지도적 학습인데, 이 말은 우리가 어떤 문장이 나와야 되는지에 대해서 정의할 필요가 없는 대신 신경망이 스스로 이를 찾는 방법입니다. 신경망에 손실을 추가해서 attention 메커니즘이 정확한 문장을 찾는지를 확인해보시기 바랍니다.\n",
        "\n",
        "* Coattention: 입력 문장에 대해서 단순히 넣는 것 대신에, 몇몇 연구자들이 “Dynamic coattention networks”라고 부르는 것에 성공했는데, 이는 행렬을 집어넣을 때 두 문장에 대한 위치를 동시에 나타내주는 방법입니다.\n",
        "대체 벡터화 scheme과 소스들: 한번 문장과 벡터 사이에 지능화된 매핑 형태를 만들어보시거나 다른 데이터 집합을 사용해보시기 바랍니다. GloVe는 각각 300차원을 가지는 8400억개 독특한 토큰 데이터를 제공합니다. \n",
        "\n",
        "\n",
        "<p><em>This post is a collaboration between O'Reilly and </em><a href=\"https://www.tensorflow.org/\"><em>TensorFlow</em></a><em>. </em><a href=\"http://www.oreilly.com/about/editorial_independence.html\"><em>See our statement of editorial independence</em></a><em>.</em></p>"
      ]
    }
  ]
}